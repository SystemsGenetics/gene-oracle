{
    "lr": {
        "solver": "lbfgs",
        "max_iter": 1000,
        "multi_class": "auto"
    },
    "mlp-skl": {
        "hidden_layer_sizes": [512, 256, 128],
        "activation": "relu",
        "solver": "adam",
        "batch_size": 256,
        "learning_rate_init": 0.001,
        "max_iter": 10000
    },
    "mlp-tf": {
        "layers": [512, 256, 128],
        "activations": ["relu", "relu", "relu"],
        "dropout": false,
        "lr": 0.001,
        "epochs": 50,
        "batch_size": 256,
        "verbose": false
    },
    "rf": {
        "n_estimators": 100
    },
    "svm": {
        "kernel": "rbf",
        "gamma": "scale"
    }
}
